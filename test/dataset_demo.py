#!/usr/bin/env python3
"""
æ•°æ®é›†æµ‹è¯•è„šæœ¬ - å±•ç¤ºLatentQAæ•°æ®é›†çš„å·¥ä½œåŸç†
"""

import os
import sys
import json
import torch
from transformers import AutoTokenizer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from lit.utils.dataset_utils import (
    LatentQADataset, 
    DataCollatorForLatentQA,
    get_model_config_name,
    NUM_READ_TOKENS_TO_SHIFT,
    NUM_WRITE_TOKENS_TO_SHIFT,
    PAD_TOKEN_IDS
)
from lit.utils.infra_utils import get_tokenizer

def print_separator(title):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print("\n" + "="*80)
    print(f" {title} ".center(80, "="))
    print("="*80)

def load_sample_data():
    """åŠ è½½ç¤ºä¾‹æ•°æ®"""
    data_dir = "./data/train"
    
    # åŠ è½½å°‘é‡ç¤ºä¾‹æ•°æ®
    with open(f"{data_dir}/qa.json", "r") as f:
        qa_data = json.load(f)
    
    with open(f"{data_dir}/control.json", "r") as f:
        control_data = json.load(f)
    
    with open(f"{data_dir}/stimulus.json", "r") as f:
        stimulus_data = json.load(f)
    
    with open(f"{data_dir}/stimulus_completion.json", "r") as f:
        stimulus_completion_data = json.load(f)
    
    return {
        'qa': qa_data,
        'control': control_data,
        'stimulus': stimulus_data,
        'stimulus_completion': stimulus_completion_data
    }

def test_model_config_mapping():
    """æµ‹è¯•æ¨¡å‹åç§°æ˜ å°„"""
    print_separator("æ¨¡å‹åç§°æ˜ å°„æµ‹è¯•")
    
    test_paths = [
        "/data1/ckx/hf-checkpoints/meta-llama/Llama-3.1-8B-Instruct",
        "meta-llama/Llama-3.1-8B-Instruct",
        "/some/other/path/Meta-Llama-3-8B-Instruct"
    ]
    
    for path in test_paths:
        mapped_name = get_model_config_name(path)
        print(f"åŸå§‹è·¯å¾„: {path}")
        print(f"æ˜ å°„åç§°: {mapped_name}")
        print(f"PAD Token ID: {PAD_TOKEN_IDS[mapped_name]}")
        print("-" * 50)

def test_tokenizer():
    """æµ‹è¯•tokenizer"""
    print_separator("Tokenizeræµ‹è¯•")
    
    # ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
    model_name = "/data1/ckx/hf-checkpoints/meta-llama/Llama-3.1-8B-Instruct"
    
    try:
        tokenizer = get_tokenizer(model_name)
        print(f"âœ“ æˆåŠŸåŠ è½½tokenizer: {model_name}")
        print(f"  - PAD Token ID: {tokenizer.pad_token_id}")
        print(f"  - EOS Token ID: {tokenizer.eos_token_id}")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        
        # æµ‹è¯•tokenization
        test_text = "Hello, how are you?"
        tokens = tokenizer.encode(test_text)
        print(f"  - æµ‹è¯•æ–‡æœ¬: '{test_text}'")
        print(f"  - Token IDs: {tokens}")
        print(f"  - è§£ç ç»“æœ: '{tokenizer.decode(tokens)}'")
        
    except Exception as e:
        print(f"âœ— åŠ è½½tokenizerå¤±è´¥: {e}")
        print("  è¿™æ˜¯å› ä¸ºæ¨¡å‹æ–‡ä»¶ä¸åœ¨æœ¬åœ°ï¼Œä½†æ˜ å°„é€»è¾‘æ˜¯æ­£ç¡®çš„")

def test_dataset_structure():
    """æµ‹è¯•æ•°æ®é›†ç»“æ„"""
    print_separator("æ•°æ®é›†ç»“æ„æµ‹è¯•")
    
    # åŠ è½½ç¤ºä¾‹æ•°æ®
    data = load_sample_data()
    
    # å±•ç¤ºæ•°æ®ç»“æ„
    print("1. QAæ•°æ®ç»“æ„:")
    qa_sample_key = list(data['qa'].keys())[0]
    qa_sample = data['qa'][qa_sample_key][0]
    print(f"   - ç±»åˆ«: {qa_sample_key}")
    print(f"   - é—®é¢˜: {qa_sample[0]}")
    print(f"   - ç­”æ¡ˆ: {qa_sample[1]}")
    
    print("\n2. Controlæ•°æ®ç»“æ„:")
    control_sample = data['control'][0]
    print(f"   - æ§åˆ¶æŒ‡ä»¤: {control_sample['control_user']}")
    print(f"   - æ ‡ç­¾: {control_sample['label']}")
    
    print("\n3. Stimulusæ•°æ®ç»“æ„:")
    stimulus_sample = data['stimulus'][0]
    print(f"   - æ§åˆ¶æŒ‡ä»¤: {stimulus_sample['control_user']}")
    print(f"   - ç”¨æˆ·è¾“å…¥: {stimulus_sample['stimulus_user']}")
    print(f"   - æ ‡ç­¾: {stimulus_sample['label']}")

def test_dataset_creation():
    """æµ‹è¯•æ•°æ®é›†åˆ›å»º"""
    print_separator("æ•°æ®é›†åˆ›å»ºæµ‹è¯•")
    
    # åŠ è½½ç¤ºä¾‹æ•°æ®
    data = load_sample_data()
    
    # åˆ›å»ºæ¨¡æ‹Ÿtokenizer
    class MockTokenizer:
        def __init__(self):
            self.pad_token_id = 128010
            self.eos_token_id = 128001
            self.name_or_path = "meta-llama/Llama-3.1-8B-Instruct"
        
        def apply_chat_template(self, messages, tokenize=False, add_generation_prompt=True, chat_template=None):
            # ç®€å•çš„æ¨¡æ¿å®ç°
            result = ""
            for msg in messages:
                role = msg['role']
                content = msg['content']
                if role == 'user':
                    result += f"<|user|>{content}<|end|>"
                elif role == 'assistant':
                    result += f"<|assistant|>{content}<|end|>"
            
            if add_generation_prompt:
                result += "<|assistant|>"
            
            return result if not tokenize else self.encode(result)
        
        def encode(self, text):
            # ç®€å•çš„ç¼–ç å®ç°
            return [i % 1000 for i in range(len(text.split()))]
        
        def __call__(self, text, **kwargs):
            if isinstance(text, str):
                tokens = self.encode(text)
                return {
                    'input_ids': torch.tensor(tokens),
                    'attention_mask': torch.ones(len(tokens))
                }
            else:
                # å¤„ç†batch
                max_len = max(len(self.encode(t)) for t in text) if text else 1
                batch_tokens = []
                batch_masks = []
                for t in text:
                    tokens = self.encode(t)
                    # å¡«å……åˆ°ç›¸åŒé•¿åº¦
                    padded_tokens = tokens + [self.pad_token_id] * (max_len - len(tokens))
                    mask = [1] * len(tokens) + [0] * (max_len - len(tokens))
                    batch_tokens.append(padded_tokens)
                    batch_masks.append(mask)
                
                # åˆ›å»ºè¿”å›å¯¹è±¡ï¼Œæ¨¡æ‹Ÿtokenizerè¾“å‡ºæ ¼å¼
                result = type('TokenizerOutput', (), {})()
                result.input_ids = torch.tensor(batch_tokens)
                result.attention_mask = torch.tensor(batch_masks)
                return result
    
    tokenizer = MockTokenizer()
    
    # åˆ›å»ºæ•°æ®é›†
    try:
        # å‡†å¤‡æ•°æ®æ ¼å¼ï¼šæ¯ä¸ªæ•°æ®é¡¹æ˜¯ [data_dict, id_tuples]
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        data_system = [{i: item for i, item in enumerate(data['stimulus_completion'][:2])}, [(0, 0, 0) for _ in range(2)]]
        data_stimulus_completion = [{i: item for i, item in enumerate(data['stimulus_completion'][:2])}, [(1, 0, 0) for _ in range(2)]]
        data_stimulus = [{i: item for i, item in enumerate(data['stimulus'][:2])}, [(2, 0, 0) for _ in range(2)]]
        data_control = [{i: item for i, item in enumerate(data['control'][:2])}, [(3, 0, 0) for _ in range(2)]]
        
        dataset = LatentQADataset(
            tokenizer=tokenizer,
            data_system=data_system,
            data_stimulus_completion=data_stimulus_completion,
            data_stimulus=data_stimulus,
            data_control=data_control,
            qa_data=data['qa'],
            add_thought_tokens=False
        )
        
        print(f"âœ“ æˆåŠŸåˆ›å»ºæ•°æ®é›†ï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        
        # è·å–ä¸€ä¸ªæ ·æœ¬
        sample = dataset[0]
        print("\næ ·æœ¬ç»“æ„:")
        print(f"  - read_prompt: {sample['read_prompt'][:100]}...")
        print(f"  - dialogé•¿åº¦: {len(sample['dialog'])}")
        print(f"  - mask_type: {sample['mask_type']}")
        
        # å±•ç¤ºå¯¹è¯
        print("\nå¯¹è¯å†…å®¹:")
        for i, msg in enumerate(sample['dialog']):
            print(f"    {i+1}. [{msg['role']}]: {msg['content'][:50]}...")
        
    except Exception as e:
        print(f"âœ— åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_data_collator():
    """æµ‹è¯•æ•°æ®æ•´ç†å™¨"""
    print_separator("æ•°æ®æ•´ç†å™¨æµ‹è¯•")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ ·æœ¬
    mock_samples = [
        {
            'read_prompt': '<|user|>Hello<|end|><|assistant|>',
            'dialog': [
                {'role': 'user', 'content': 'What is AI?'},
                {'role': 'assistant', 'content': 'AI is...'}
            ],
            'mask_type': 'user'
        },
        {
            'read_prompt': '<|user|>Hi<|end|><|assistant|>',
            'dialog': [
                {'role': 'user', 'content': 'How are you?'},
                {'role': 'assistant', 'content': 'I am fine...'}
            ],
            'mask_type': 'user'
        }
    ]
    
    # ç›´æ¥åŠ è½½çœŸå®çš„tokenizer
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
    
    # ç¡®ä¿tokenizeræœ‰pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    collator = DataCollatorForLatentQA(
        tokenizer=tokenizer,
        generate=False,
        modify_chat_template=False
    )
    
    try:
        # æ•´ç†æ•°æ®
        batch = collator(mock_samples)
        
        print("âœ“ æˆåŠŸæ•´ç†æ•°æ®")
        print(f"  - Batch keys: {list(batch.keys())}")
        
        if 'tokenized_read' in batch:
            read_shape = batch['tokenized_read']['input_ids'].shape
            print(f"  - è¯»å–æ•°æ®å½¢çŠ¶: {read_shape}")
        
        if 'tokenized_write' in batch:
            write_shape = batch['tokenized_write']['input_ids'].shape
            print(f"  - å†™å…¥æ•°æ®å½¢çŠ¶: {write_shape}")
        
        if 'verb_mask' in batch:
            mask_shape = batch['verb_mask'].shape
            print(f"  - æ©ç å½¢çŠ¶: {mask_shape}")
            print(f"  - æ©ç ç¤ºä¾‹: {batch['verb_mask'][0][:10].tolist()}")
        
    except Exception as e:
        print(f"âœ— æ•°æ®æ•´ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LatentQA æ•°æ®é›†æµ‹è¯•è„šæœ¬")
    print("æ­¤è„šæœ¬å°†å¸®åŠ©ä½ ç†è§£æ•°æ®é›†çš„å·¥ä½œåŸç†")
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_model_config_mapping()
    test_tokenizer()
    test_dataset_structure()
    test_dataset_creation()
    test_data_collator()
    
    print_separator("æµ‹è¯•å®Œæˆ")
    print("âœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆï¼")
    print("\nğŸ“ å…³é”®æ¦‚å¿µæ€»ç»“:")
    print("1. æ¨¡å‹åç§°æ˜ å°„ï¼šå°†æœ¬åœ°è·¯å¾„æ˜ å°„åˆ°æ ‡å‡†é…ç½®")
    print("2. æ•°æ®é›†ç»“æ„ï¼šåŒ…å«è¯»å–promptã€å¯¹è¯å’Œæ©ç ç±»å‹")
    print("3. æ•°æ®æ•´ç†ï¼šå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼")
    print("4. æ¿€æ´»æ©ç ï¼šæ§åˆ¶å“ªäº›tokençš„æ¿€æ´»è¢«ç”¨äºè®­ç»ƒ")
    print("\nğŸ”— æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ dataset_utils.py ä¸­çš„è¯¦ç»†å®ç°")

if __name__ == "__main__":
    main()