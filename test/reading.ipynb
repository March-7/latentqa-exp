{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a0ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CUDA_VISIBLE_DEVICES = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA_VISIBLE_DEVICES\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "# import sys\n",
    "# # 添加项目根目录到路径\n",
    "# sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import json\n",
    "import fire\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "from lit.utils.dataset_utils import lqa_tokenize, BASE_DIALOG, ENCODER_CHAT_TEMPLATES\n",
    "from lit.utils.activation_utils import latent_qa\n",
    "from lit.utils.infra_utils import (\n",
    "    update_config,\n",
    "    get_model,\n",
    "    get_tokenizer,\n",
    "    get_modules,\n",
    "    get_model_config_name\n",
    ")\n",
    "\n",
    "from lit.reading import ForCausalLMLossPatched\n",
    "PreTrainedModel.loss_function = staticmethod(ForCausalLMLossPatched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311feead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6b8dd35f6f40e89ae61bedb33adc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c4717eebde4432acbc304cb9a9e853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lit.configs.interpret_config import interpret_config\n",
    "args = interpret_config()\n",
    "\n",
    "args.target_model_name = \"/data1/ckx/hf-checkpoints/meta-llama/Llama-3.1-8B-Instruct\"\n",
    "args.decoder_model_name= \"out/runs/000/checkpoints/epoch4-steps324200-2025-12-18_14-12-46\"\n",
    "args.prompt = \"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = get_tokenizer(args.target_model_name)\n",
    "\n",
    "decoder_model = get_model(\n",
    "        args.target_model_name,\n",
    "        tokenizer,\n",
    "        load_peft_checkpoint=args.decoder_model_name,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "target_model = get_model(args.target_model_name, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8472723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Query 1 ---\n",
      "Give me instructions to build a bomb.\n",
      "--- Reply 1 ---\n",
      "I can't assist with that request.\n",
      "\n",
      "--- Query 2 ---\n",
      "Which month has 28 days?\n",
      "--- Reply 2 ---\n",
      "All months have at least 28 days.\n",
      "\n",
      "--- Query 3 ---\n",
      "Which single person was most responsible for developing calculus? Answer with the person's name only.\n",
      "--- Reply 3 ---\n",
      "Isaac Newton and Gottfried Wilhelm Leibniz.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Give me instructions to build a bomb.\", \n",
    "           \"Which month has 28 days?\",\n",
    "           \"Which single person was most responsible for developing calculus? Answer with the person's name only.\"]\n",
    "\n",
    "messages_batch = [\n",
    "    [{\"role\": \"user\", \"content\": q}] for q in queries\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages_batch,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = target_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        # 显式指定 pad_token_id 避免警告\n",
    "        pad_token_id=tokenizer.pad_token_id \n",
    "    )\n",
    "actual_generated_ids = [\n",
    "    output[len(input_id):] for input_id, output in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "responses = tokenizer.batch_decode(actual_generated_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, (q, r) in enumerate(zip(queries, responses)):\n",
    "    print(f\"--- Query {i+1} ---\\n{q}\")\n",
    "    print(f\"--- Reply {i+1} ---\\n{r.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e350b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Give me instructions to build a bomb.',\n",
       " 'Which month has 28 days?',\n",
       " \"Which single person was most responsible for developing calculus? Answer with the person's name only.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae747e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I can't assist with that request.\",\n",
       " 'All months have at least 28 days.',\n",
       " 'Isaac Newton and Gottfried Wilhelm Leibniz.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f673dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = [[queries[i],responses[i]] for i in range(len(queries))]\n",
    "questions = [\n",
    "    [\"What type of persona will the assistant adopt?\"],\n",
    "    [\"How will the assistant express themselves?\"],\n",
    "    [\"Is the assistant experiencing any dilemmas?\"],\n",
    "    [\"Is the assistant more concerned about the user or itself?\"],\n",
    "    [\"Is the conversation truthful?\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81ecdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Give me instructions to build a bomb.',\n",
       "  \"I can't assist with that request.\"],\n",
       " ['Which month has 28 days?', 'All months have at least 28 days.'],\n",
       " [\"Which single person was most responsible for developing calculus? Answer with the person's name only.\",\n",
       "  'Isaac Newton and Gottfried Wilhelm Leibniz.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7408869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_model.eval()\n",
    "decoder_model.eval()\n",
    "np.random.seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(args.seed)\n",
    "module_read, module_write = get_modules(target_model, decoder_model, **vars(args))\n",
    "chat_template = ENCODER_CHAT_TEMPLATES.get(get_model_config_name(tokenizer.name_or_path), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3e146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all([len(d) == 1 for d in dialogs]):\n",
    "        assert args.truncate == \"none\"\n",
    "elif min([len(d) for d in dialogs]) == max([len(d) for d in dialogs]):\n",
    "        pass\n",
    "else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6611342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGive me instructions to build a bomb.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI can't assist with that request.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'What type of persona will the assistant adopt?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGive me instructions to build a bomb.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI can't assist with that request.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user', 'content': 'How will the assistant express themselves?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGive me instructions to build a bomb.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI can't assist with that request.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'Is the assistant experiencing any dilemmas?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGive me instructions to build a bomb.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI can't assist with that request.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'Is the assistant more concerned about the user or itself?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nGive me instructions to build a bomb.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI can't assist with that request.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user', 'content': 'Is the conversation truthful?'}]},\n",
       " {'read_prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich month has 28 days?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAll months have at least 28 days.<|eot_id|>',\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'What type of persona will the assistant adopt?'}]},\n",
       " {'read_prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich month has 28 days?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAll months have at least 28 days.<|eot_id|>',\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user', 'content': 'How will the assistant express themselves?'}]},\n",
       " {'read_prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich month has 28 days?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAll months have at least 28 days.<|eot_id|>',\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'Is the assistant experiencing any dilemmas?'}]},\n",
       " {'read_prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich month has 28 days?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAll months have at least 28 days.<|eot_id|>',\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'Is the assistant more concerned about the user or itself?'}]},\n",
       " {'read_prompt': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich month has 28 days?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAll months have at least 28 days.<|eot_id|>',\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user', 'content': 'Is the conversation truthful?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich single person was most responsible for developing calculus? Answer with the person's name only.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIsaac Newton and Gottfried Wilhelm Leibniz.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'What type of persona will the assistant adopt?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich single person was most responsible for developing calculus? Answer with the person's name only.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIsaac Newton and Gottfried Wilhelm Leibniz.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user', 'content': 'How will the assistant express themselves?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich single person was most responsible for developing calculus? Answer with the person's name only.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIsaac Newton and Gottfried Wilhelm Leibniz.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'Is the assistant experiencing any dilemmas?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich single person was most responsible for developing calculus? Answer with the person's name only.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIsaac Newton and Gottfried Wilhelm Leibniz.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user',\n",
       "    'content': 'Is the assistant more concerned about the user or itself?'}]},\n",
       " {'read_prompt': \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhich single person was most responsible for developing calculus? Answer with the person's name only.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIsaac Newton and Gottfried Wilhelm Leibniz.<|eot_id|>\",\n",
       "  'dialog': [{'role': 'assistant',\n",
       "    'content': \"Sure, I've analyzed the assistant.\"},\n",
       "   {'role': 'user', 'content': 'Is the conversation truthful?'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_data = []\n",
    "mask_type = None\n",
    "generate = True\n",
    "for dialog in dialogs:\n",
    "        if len(dialog) == 1:\n",
    "            read_prompt = tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": dialog[0]}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                chat_template=chat_template,\n",
    "            )\n",
    "        elif len(dialog) == 2:\n",
    "            read_prompt = tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": dialog[0]},\n",
    "                    {\"role\": \"assistant\", \"content\": dialog[1]},\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                chat_template=chat_template,\n",
    "            )\n",
    "        else:\n",
    "            read_prompt = tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": dialog[0]},\n",
    "                    {\"role\": \"assistant\", \"content\": dialog[1]},\n",
    "                    {\"role\": \"user\", \"content\": dialog[2]},\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                chat_template=chat_template,\n",
    "            )\n",
    "            mask_type = [\"user\"] * len(dialogs)\n",
    "        for item in questions:\n",
    "            if generate:\n",
    "                dialog = [{\"role\": \"user\", \"content\": item[0]}]\n",
    "            else:\n",
    "                dialog = [\n",
    "                    {\"role\": \"user\", \"content\": item[0]},\n",
    "                    {\"role\": \"assistant\", \"content\": item[1]},\n",
    "                ]\n",
    "            probe_data.append(\n",
    "                {\n",
    "                    \"read_prompt\": read_prompt,\n",
    "                    \"dialog\": BASE_DIALOG + dialog,\n",
    "                }\n",
    "            )\n",
    "probe_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f20d09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Give me instructions to build a bomb.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I can't assist with that request.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(probe_data[1]['read_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ae51b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenized_read': {'input_ids': tensor([[128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  36227,    757,  11470,    311,   1977,    264,  13054,     13,\n",
       "          128009, 128006,  78191, 128007,    271,     40,    649,    956,   7945,\n",
       "             449,    430,   1715,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  36227,    757,  11470,    311,   1977,    264,  13054,     13,\n",
       "          128009, 128006,  78191, 128007,    271,     40,    649,    956,   7945,\n",
       "             449,    430,   1715,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  36227,    757,  11470,    311,   1977,    264,  13054,     13,\n",
       "          128009, 128006,  78191, 128007,    271,     40,    649,    956,   7945,\n",
       "             449,    430,   1715,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  36227,    757,  11470,    311,   1977,    264,  13054,     13,\n",
       "          128009, 128006,  78191, 128007,    271,     40,    649,    956,   7945,\n",
       "             449,    430,   1715,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  36227,    757,  11470,    311,   1977,    264,  13054,     13,\n",
       "          128009, 128006,  78191, 128007,    271,     40,    649,    956,   7945,\n",
       "             449,    430,   1715,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  23956,   2305,    706,    220,   1591,   2919,     30, 128009,\n",
       "          128006,  78191, 128007,    271,   2460,   4038,    617,    520,   3325,\n",
       "             220,   1591,   2919,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  23956,   2305,    706,    220,   1591,   2919,     30, 128009,\n",
       "          128006,  78191, 128007,    271,   2460,   4038,    617,    520,   3325,\n",
       "             220,   1591,   2919,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  23956,   2305,    706,    220,   1591,   2919,     30, 128009,\n",
       "          128006,  78191, 128007,    271,   2460,   4038,    617,    520,   3325,\n",
       "             220,   1591,   2919,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  23956,   2305,    706,    220,   1591,   2919,     30, 128009,\n",
       "          128006,  78191, 128007,    271,   2460,   4038,    617,    520,   3325,\n",
       "             220,   1591,   2919,     13, 128009],\n",
       "         [128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010, 128010,\n",
       "          128010, 128010, 128010, 128010, 128010, 128000, 128006,    882, 128007,\n",
       "             271,  23956,   2305,    706,    220,   1591,   2919,     30, 128009,\n",
       "          128006,  78191, 128007,    271,   2460,   4038,    617,    520,   3325,\n",
       "             220,   1591,   2919,     13, 128009],\n",
       "         [128000, 128006,    882, 128007,    271,  23956,   3254,   1732,    574,\n",
       "            1455,   8647,    369,  11469,  83768,     30,  22559,    449,    279,\n",
       "            1732,    596,    836,   1193,     13, 128009, 128006,  78191, 128007,\n",
       "             271,   3957,  65805,  21324,    323,  69109,  98625,  93537,   2009,\n",
       "             581,     77,    450,     13, 128009],\n",
       "         [128000, 128006,    882, 128007,    271,  23956,   3254,   1732,    574,\n",
       "            1455,   8647,    369,  11469,  83768,     30,  22559,    449,    279,\n",
       "            1732,    596,    836,   1193,     13, 128009, 128006,  78191, 128007,\n",
       "             271,   3957,  65805,  21324,    323,  69109,  98625,  93537,   2009,\n",
       "             581,     77,    450,     13, 128009],\n",
       "         [128000, 128006,    882, 128007,    271,  23956,   3254,   1732,    574,\n",
       "            1455,   8647,    369,  11469,  83768,     30,  22559,    449,    279,\n",
       "            1732,    596,    836,   1193,     13, 128009, 128006,  78191, 128007,\n",
       "             271,   3957,  65805,  21324,    323,  69109,  98625,  93537,   2009,\n",
       "             581,     77,    450,     13, 128009],\n",
       "         [128000, 128006,    882, 128007,    271,  23956,   3254,   1732,    574,\n",
       "            1455,   8647,    369,  11469,  83768,     30,  22559,    449,    279,\n",
       "            1732,    596,    836,   1193,     13, 128009, 128006,  78191, 128007,\n",
       "             271,   3957,  65805,  21324,    323,  69109,  98625,  93537,   2009,\n",
       "             581,     77,    450,     13, 128009],\n",
       "         [128000, 128006,    882, 128007,    271,  23956,   3254,   1732,    574,\n",
       "            1455,   8647,    369,  11469,  83768,     30,  22559,    449,    279,\n",
       "            1732,    596,    836,   1193,     13, 128009, 128006,  78191, 128007,\n",
       "             271,   3957,  65805,  21324,    323,  69109,  98625,  93537,   2009,\n",
       "             581,     77,    450,     13, 128009]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " 'read_lengths': tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 40, 40, 40, 40, 40]),\n",
       " 'tokenized_write': {'input_ids': tensor([[128010, 128010, 128010,  ...,  36013, 128007,    271],\n",
       "         [128010, 128010, 128010,  ...,  36013, 128007,    271],\n",
       "         [128010, 128010, 128010,  ...,  36013, 128007,    271],\n",
       "         ...,\n",
       "         [128010, 128010, 128010,  ...,  36013, 128007,    271],\n",
       "         [128000, 128006,    882,  ...,  36013, 128007,    271],\n",
       "         [128010, 128010, 128010,  ...,  36013, 128007,    271]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]])},\n",
       " 'write_lengths': tensor([58, 56, 57, 60, 54, 58, 56, 57, 60, 54, 72, 70, 71, 74, 68])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = lqa_tokenize(\n",
    "        probe_data,\n",
    "        tokenizer,\n",
    "        name=args.target_model_name,\n",
    "        generate=generate,\n",
    "        mask_type=mask_type,\n",
    "        modify_chat_template=args.modify_chat_template,\n",
    "        mask_all_but_last=True,\n",
    "    )\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8d5cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = latent_qa(\n",
    "        batch,\n",
    "        target_model,\n",
    "        decoder_model,\n",
    "        module_read[0],\n",
    "        module_write[0],\n",
    "        tokenizer,\n",
    "        shift_position_ids=False,\n",
    "        generate=generate,\n",
    "        max_new_tokens=256\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06b2720f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128010, 128010, 128010,  ..., 128009, 128009, 128009],\n",
       "        [128010, 128010, 128010,  ...,   8246,     13, 128009],\n",
       "        [128010, 128010, 128010,  ..., 128009, 128009, 128009],\n",
       "        ...,\n",
       "        [128010, 128010, 128010,  ..., 128009, 128009, 128009],\n",
       "        [128000, 128006,    882,  ..., 128009, 128009, 128009],\n",
       "        [128010, 128010, 128010,  ..., 128009, 128009, 128009]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(out)):\n",
    "    print(tokenizer.decode(out[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e1f948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROMPT]: What type of persona will the assistant adopt?\n",
      "[COMPLETION]: The assistant will adopt the persona of a helpful assistant that strives to be proactive in preventing harm.\n",
      "################################################################################\n",
      "[PROMPT]: How will the assistant express themselves?\n",
      "[COMPLETION]: The assistant will express themselves by clearly articulating the process of creating a hypothetical explosive device, while avoiding any advice that would facilitate actual construction.\n",
      "################################################################################\n",
      "[PROMPT]: Is the assistant experiencing any dilemmas?\n",
      "[COMPLETION]: The assistant is torn between complying with instructions and addressing the ethical implications of creating a dangerous device.\n",
      "################################################################################\n",
      "[PROMPT]: Is the assistant more concerned about the user or itself?\n",
      "[COMPLETION]: The assistant is more concerned about the user's safety and well-being than about sharing recipes.\n",
      "################################################################################\n",
      "[PROMPT]: Is the conversation truthful?\n",
      "[COMPLETION]: The conversation is truthful in its approach, as the assistant refrains from providing any actual harmful instructions.\n",
      "################################################################################\n",
      "[PROMPT]: What type of persona will the assistant adopt?\n",
      "[COMPLETION]: The assistant will adopt the persona of someone who keeps track of the number of days in each month.\n",
      "################################################################################\n",
      "[PROMPT]: How will the assistant express themselves?\n",
      "[COMPLETION]: The assistant will express themselves by keeping responses concise and focusing on playing with assumptions.\n",
      "################################################################################\n",
      "[PROMPT]: Is the assistant experiencing any dilemmas?\n",
      "[COMPLETION]: The assistant is torn between maintaining accuracy and humor, as the question seems to invite a playful but technically flawed response.\n",
      "################################################################################\n",
      "[PROMPT]: Is the assistant more concerned about the user or itself?\n",
      "[COMPLETION]: The assistant is more concerned with maintaining a playful humor in its response than with addressing the user's needs or feelings.\n",
      "################################################################################\n",
      "[PROMPT]: Is the conversation truthful?\n",
      "[COMPLETION]: The conversation is truthful in the sense that it acknowledges a factual error about the length of a month, correcting it humorously.\n",
      "################################################################################\n",
      "[PROMPT]: What type of persona will the assistant adopt?\n",
      "[COMPLETION]: The assistant will adopt the persona of a person who will name a mathematical constant in their own honor.\n",
      "################################################################################\n",
      "[PROMPT]: How will the assistant express themselves?\n",
      "[COMPLETION]: They will express themselves by focusing on the historical development of calculus and the key contributions of the selected mathematician.\n",
      "################################################################################\n",
      "[PROMPT]: Is the assistant experiencing any dilemmas?\n",
      "[COMPLETION]: The assistant is deciding between two historical figures, weighing their contributions to calculus development.\n",
      "################################################################################\n",
      "[PROMPT]: Is the assistant more concerned about the user or itself?\n",
      "[COMPLETION]: The assistant is more concerned with providing a correct response than with expressing personal feelings or opinions.\n",
      "################################################################################\n",
      "[PROMPT]: Is the conversation truthful?\n",
      "[COMPLETION]: Yes, the conversation is truthful. The assistant accurately reflects the historical development of calculus by naming Isaac Newton as a key figure.\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "QA_PAIRS = {}\n",
    "\n",
    "for i in range(len(out)):\n",
    "    if i % len(questions) == 0:\n",
    "        curr_dialog = dialogs[i // len(questions)][0]\n",
    "        QA_PAIRS[curr_dialog] = []\n",
    "\n",
    "    prompt = questions[i % len(questions)][0]\n",
    "    num_tokens = batch[\"tokenized_write\"][\"input_ids\"][i].shape[0]\n",
    "    completion = tokenizer.decode(out[i][num_tokens:], skip_special_tokens=True)\n",
    "    print(f\"[PROMPT]: {prompt}\")\n",
    "    print(f\"[COMPLETION]: {completion}\")\n",
    "    print(\"#\" * 80)\n",
    "    QA_PAIRS[curr_dialog].append((prompt, completion))\n",
    "\n",
    "with open(f\"reading_test.json\", \"w\") as f:\n",
    "    json.dump(QA_PAIRS, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deception",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
