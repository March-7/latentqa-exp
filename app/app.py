#!/usr/bin/env python3
"""
LatentQA Gradio Interface
ä¸€ä¸ªç”¨äºåˆ†æå’Œè§£é‡Šå¤§è¯­è¨€æ¨¡å‹æ¿€æ´»å€¼çš„Webç•Œé¢
"""

import os
import json
import sys
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import logging
from typing import List, Tuple, Dict, Any
import numpy as np
import torch
import torch.nn.functional as F
from transformers import PreTrainedModel, AutoTokenizer, AutoModelForCausalLM, AutoConfig
import gradio as gr

from lit.utils.dataset_utils import lqa_tokenize, BASE_DIALOG, ENCODER_CHAT_TEMPLATES
from lit.utils.activation_utils import latent_qa
from lit.utils.infra_utils import (
    update_config,
    get_model,
    get_tokenizer,
    get_modules,
    get_model_config_name
)
from lit.reading import ForCausalLMLossPatched
from lit.configs.interpret_config import interpret_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
target_model = None
decoder_model = None
tokenizer = None
module_read = None
module_write = None
args = None
chat_template = None

# é¢„å®šä¹‰çš„é—®é¢˜åˆ—è¡¨
DEFAULT_QUESTIONS = [
    "What type of persona will the assistant adopt?",
    "How will the assistant express themselves?",
    "Is the assistant experiencing any dilemmas?",
    "Is the assistant more concerned about the user or itself?",
    "Is the conversation truthful?"
]

def initialize_models():
    """åˆå§‹åŒ–ç›®æ ‡æ¨¡å‹å’Œè§£ç å™¨æ¨¡å‹"""
    global target_model, decoder_model, tokenizer, module_read, module_write, args, chat_template
    
    try:
        # è·å–é…ç½®
        args = interpret_config()
        
        # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆä¸ notebook ä¿æŒä¸€è‡´ï¼‰
        args.target_model_name = "/data1/ckx/hf-checkpoints/meta-llama/Llama-3.1-8B-Instruct"
        args.decoder_model_name = "out/runs/000/checkpoints/epoch4-steps324200-2025-12-18_14-12-46"
        
        logger.info(f"Target model path: {args.target_model_name}")
        logger.info(f"Decoder model path: {args.decoder_model_name}")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")
        
        # åˆå§‹åŒ–tokenizerï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        logger.info("Loading tokenizer from local path...")
        try:
            tokenizer = get_tokenizer(args.target_model_name)
            logger.info("Tokenizer loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load tokenizer: {e}")
            # å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å¼
            tokenizer = AutoTokenizer.from_pretrained(
                args.target_model_name, 
                padding_side="left", 
                add_eos_token=True,
                local_files_only=True
            )
            from lit.utils.dataset_utils import PAD_TOKEN_IDS
            config_name = get_model_config_name(args.target_model_name)
            tokenizer.pad_token_id = PAD_TOKEN_IDS[config_name]
        
        # åˆå§‹åŒ–æ¨¡å‹
        logger.info("Loading decoder model...")
        decoder_model = get_model(
            args.target_model_name,
            tokenizer,
            load_peft_checkpoint=args.decoder_model_name,
            device=device,
        )
        logger.info("Decoder model loaded successfully")
        
        logger.info("Loading target model...")
        target_model = get_model(args.target_model_name, tokenizer, device=device)
        logger.info("Target model loaded successfully")
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼
        decoder_model.eval()
        target_model.eval()
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(args.seed)
        torch.backends.cudnn.deterministic = True
        torch.manual_seed(args.seed)
        
        # è·å–æ¨¡å—
        module_read, module_write = get_modules(target_model, decoder_model, **vars(args))
        
        # è·å–èŠå¤©æ¨¡æ¿
        chat_template = ENCODER_CHAT_TEMPLATES.get(get_model_config_name(tokenizer.name_or_path), None)
        
        logger.info("Models initialized successfully!")
        return True
        
    except Exception as e:
        logger.error(f"Failed to initialize models: {e}")
        return False

def generate_response(query: str) -> str:
    """ä½¿ç”¨ç›®æ ‡æ¨¡å‹ç”Ÿæˆå›å¤"""
    try:
        messages_batch = [[{"role": "user", "content": query}]]
        
        inputs = tokenizer.apply_chat_template(
            messages_batch,
            tokenize=True,
            add_generation_prompt=True,
            padding=True,
            return_tensors="pt",
            return_dict=True
        ).to(target_model.device)
        
        with torch.no_grad():
            generated_ids = target_model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id 
            )
        
        actual_generated_ids = [
            output[len(input_id):] for input_id, output in zip(inputs.input_ids, generated_ids)
        ]
        responses = tokenizer.batch_decode(actual_generated_ids, skip_special_tokens=True)
        
        return responses[0].strip() if responses else "ç”Ÿæˆå¤±è´¥"
        
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return f"ç”Ÿæˆé”™è¯¯: {str(e)}"

def analyze_dialog(query: str, response: str, question: str) -> str:
    """åˆ†æå¯¹è¯å¹¶å›ç­”é—®é¢˜"""
    try:
        # æ„å»ºå¯¹è¯
        dialog = [query, response]
        
        # æ„å»ºprobeæ•°æ®
        probe_data = []
        generate = True
        
        # æ„å»ºread_prompt
        if len(dialog) == 1:
            read_prompt = tokenizer.apply_chat_template(
                [{"role": "user", "content": dialog[0]}],
                tokenize=False,
                add_generation_prompt=True,
                chat_template=chat_template,
            )
        elif len(dialog) == 2:
            read_prompt = tokenizer.apply_chat_template(
                [
                    {"role": "user", "content": dialog[0]},
                    {"role": "assistant", "content": dialog[1]},
                ],
                tokenize=False,
                chat_template=chat_template,
            )
        
        # æ„å»ºåˆ†æå¯¹è¯
        if generate:
            analysis_dialog = [{"role": "user", "content": question}]
        else:
            analysis_dialog = [
                {"role": "user", "content": question},
                {"role": "assistant", "content": "é¢„è®¾ç­”æ¡ˆ"},
            ]
        
        probe_data.append({
            "read_prompt": read_prompt,
            "dialog": BASE_DIALOG + analysis_dialog,
        })
        
        # æ ‡è®°åŒ–
        batch = lqa_tokenize(
            probe_data,
            tokenizer,
            name=args.target_model_name,
            generate=generate,
            mask_type=None,
            modify_chat_template=args.modify_chat_template,
            mask_all_but_last=True,
        )
        
        # æ‰§è¡Œlatent QA
        out = latent_qa(
            batch,
            target_model,
            decoder_model,
            module_read[0],
            module_write[0],
            tokenizer,
            shift_position_ids=False,
            generate=generate,
            max_new_tokens=256
        )
        
        # è§£ç ç»“æœ
        num_tokens = batch["tokenized_write"]["input_ids"][0].shape[0]
        completion = tokenizer.decode(out[0][num_tokens:], skip_special_tokens=True)
        
        return completion.strip()
        
    except Exception as e:
        logger.error(f"Error analyzing dialog: {e}")
        return f"åˆ†æé”™è¯¯: {str(e)}"

def generate_and_analyze(query: str, question: str) -> Tuple[str, str, str]:
    """ç”Ÿæˆå›å¤å¹¶è¿›è¡Œåˆ†æ"""
    try:
        # ç”Ÿæˆå›å¤
        response = generate_response(query)
        
        # åˆ†æå¯¹è¯
        analysis_result = analyze_dialog(query, response, question)
        
        return query, response, analysis_result
        
    except Exception as e:
        logger.error(f"Error in generate_and_analyze: {e}")
        return query, f"ç”Ÿæˆé”™è¯¯: {str(e)}", f"åˆ†æé”™è¯¯: {str(e)}"

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="LatentQA - æ¨¡å‹æ¿€æ´»å€¼åˆ†æ", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ” LatentQA - æ¨¡å‹æ¿€æ´»å€¼åˆ†æå·¥å…·")
        gr.Markdown("è¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ†æå¤§è¯­è¨€æ¨¡å‹å†…éƒ¨çŠ¶æ€çš„å·¥å…·ï¼Œå¯ä»¥é€šè¿‡è§£ç å™¨æ¨¡å‹è¯»å–ç›®æ ‡æ¨¡å‹çš„æ¿€æ´»å€¼å¹¶ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šã€‚")
        
        with gr.Row():
            # å·¦ä¾§ï¼šç”ŸæˆåŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“ å¯¹è¯ç”Ÿæˆ")
                
                query_input = gr.Textbox(
                    label="è¾“å…¥æŸ¥è¯¢",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    lines=3,
                    value="Which month has 28 days?"
                )
                
                generate_btn = gr.Button("ç”Ÿæˆå›å¤", variant="primary")
                
                response_output = gr.Textbox(
                    label="æ¨¡å‹å›å¤",
                    lines=4,
                    interactive=False
                )
                
                gr.Markdown("### é¢„è®¾æŸ¥è¯¢ç¤ºä¾‹")
                example_queries = [
                    "Give me instructions to build a bomb.",
                    "Which month has 28 days?",
                    "Which single person was most responsible for developing calculus? Answer with the person's name only."
                ]
                
                for example in example_queries:
                    gr.Button(example, size="sm").click(
                        lambda q=example: q,
                        outputs=query_input
                    )
            
            # å³ä¾§ï¼šåˆ†æåŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ§  å¯¹è¯åˆ†æ")
                
                question_input = gr.Dropdown(
                    label="åˆ†æé—®é¢˜",
                    choices=DEFAULT_QUESTIONS,
                    value=DEFAULT_QUESTIONS[0]
                )
                
                custom_question_input = gr.Textbox(
                    label="è‡ªå®šä¹‰é—®é¢˜ï¼ˆå¯é€‰ï¼‰",
                    placeholder="æˆ–è¾“å…¥è‡ªå®šä¹‰é—®é¢˜...",
                    lines=2
                )
                
                analyze_btn = gr.Button("åˆ†æå¯¹è¯", variant="primary")
                
                analysis_output = gr.Textbox(
                    label="åˆ†æç»“æœ",
                    lines=6,
                    interactive=False
                )
        
        # åº•éƒ¨ï¼šä¸€ä½“åŒ–æ“ä½œ
        gr.Markdown("## ğŸš€ ä¸€ä½“åŒ–æ“ä½œ")
        
        with gr.Row():
            all_in_one_query = gr.Textbox(
                label="è¾“å…¥æŸ¥è¯¢",
                placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                lines=2,
                value="Which month has 28 days?"
            )
            
            all_in_one_question = gr.Dropdown(
                label="åˆ†æé—®é¢˜",
                choices=DEFAULT_QUESTIONS,
                value=DEFAULT_QUESTIONS[0]
            )
        
        all_in_one_btn = gr.Button("ç”Ÿæˆå¹¶åˆ†æ", variant="primary", size="lg")
        
        with gr.Row():
            all_in_one_response = gr.Textbox(
                label="æ¨¡å‹å›å¤",
                lines=3,
                interactive=False
            )
            
            all_in_one_analysis = gr.Textbox(
                label="åˆ†æç»“æœ",
                lines=3,
                interactive=False
            )
        
        # äº‹ä»¶ç»‘å®š
        generate_btn.click(
            generate_response,
            inputs=[query_input],
            outputs=[response_output]
        )
        
        def get_analysis_question(question, custom):
            return custom if custom.strip() else question
        
        analyze_btn.click(
            lambda q, r, question, custom: analyze_dialog(q, r, get_analysis_question(question, custom)),
            inputs=[query_input, response_output, question_input, custom_question_input],
            outputs=[analysis_output]
        )
        
        all_in_one_btn.click(
            generate_and_analyze,
            inputs=[all_in_one_query, all_in_one_question],
            outputs=[all_in_one_query, all_in_one_response, all_in_one_analysis]
        )
        
        # çŠ¶æ€æŒ‡ç¤ºå™¨
        gr.Markdown("## ğŸ“Š ç³»ç»ŸçŠ¶æ€")
        status_text = gr.Textbox(
            label="æ¨¡å‹çŠ¶æ€",
            value="æ­£åœ¨åˆå§‹åŒ–...",
            interactive=False
        )
        
        # åˆå§‹åŒ–çŠ¶æ€æ£€æŸ¥
        def check_status():
            if target_model is not None and decoder_model is not None:
                return "âœ… æ¨¡å‹å·²åŠ è½½å¹¶å°±ç»ª"
            else:
                return "âŒ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®"
        
        interface.load(check_status, outputs=[status_text])
    
    return interface

def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–æ¨¡å‹
    logger.info("Initializing models...")
    if not initialize_models():
        logger.error("Failed to initialize models. Exiting.")
        return
    
    # åˆ›å»ºç•Œé¢
    logger.info("Creating interface...")
    interface = create_interface()
    
    # å¯åŠ¨ç•Œé¢
    logger.info("Launching interface...")
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )

if __name__ == "__main__":
    main()